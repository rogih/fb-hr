---
title: Kaggle - Facebook Recruiting IV: Human or Robot?
author: "Higor Cotta"
date: "23/07/2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(scipen = 9999)
library(readr)
library(data.table)
library(plyr)
library(dplyr)
library(ggplot2)
library(caret)

EstatDescr <- function(dado, coluna = F, ingles = T) {
  sumario <- (summary(dado))
  res <- cbind(t(as.vector(sumario)), var(dado), sd(dado))
  colnames(res) <- c("Min.", "1st Qu.", "Median", "Mean", "3rd Qu.", "Max.", "Var.", "St. dev.")
  if (!ingles) {
    colnames(res) <- c("Mín.", "1º Quart.", "Mediana", "Média", "3º Quart.", "Máx.", "Var.", "Desv.")
  }
  if (!coluna) {
    return(t(res))
  }
  return(res)
}

differences <- function(df) {
  df <- df[order(df$time), ]
  df$time <- c(0, diff(df$time))
  df
}


bids.data <- read_csv("bids.csv")
test.data <- read_csv("test.csv")
train.data <- read_csv("train.csv")
```

# The problem
This is a Kaggle competition problem where the main objective is to identify bots in an online auction website. It is problematic as bots will certainly have the upper hand bidding more and faster than humans. As an expected result, bots will be the majority of winners and humans will lose confidence in betting, thus, leading to a reduction of the number of humans users. This is a tricky problem as from an overall point of view the users-base may be increasing due to an increment in the number of bots. From a simplistic point of view, it is only a binary classification problem where bidders can be either humans or bots. 


Since the competition is over, of course, the simplistic approach would be to look up top kagglers and follow their traces in order to have an unbeatable model. However, we'll approach this challenge considering a comprehensive step by step knowledge construction of the problem and dataset rather than simply jumping at creating-inserting feature in the model in a trial and error approach. Specifically, because by doing so we tackle the problem by focusing on the data/problem and not only on the tools, different machine learning classification methods aiming to win the competition.
  
How do we characterize the bidding behavior of humans and bots? We can start to address this issue by questioning how would we design a bidding-bot? We can choose two different objectives for the bot: 1-Inflate auction price and 2-Make profit. In this problem, the objective seems to be to make profit, thus some desirable yet naive behavior are listed:


+ The bot should not be detectable;
+ The bot should mimic human behavior, maybe randomly bidding on different auctions and items;
+ The bot should not bid constantly to not rise unnecessary suspicious;
+ The bot should not mindlessly bid as it would increase the final item price or increase the number of bid;
+ The bot should be able to secure the victory, thus it should bid constantly at the end of bidding time(bidding very fast with short time interval);
+ The bot should try to maximize the profit by bidding on the most profitable items;
+ The bot should be able to be turned on or off as the user wants;
+ The aggressiveness and activity of the bot should be configurable, i.e, high, medium and low;
+ The bot should be able to use different devices and IP address.


We can consider the presence and/or the absence of one or more of these features to characterize bot and human behavior.

The variables of the training dataset are: "bidder_id", "payment_account", "address" and "outcome". The outcome is a dichotomous variable indicating 1 if its a bot and 0 otherwise. The given variables for the bid dataset are: "bid_id",  "bidder_id", "auction", "merchandise", "device", "time", "country", "ip" and "url".  We also have the test data with "bidder_id", "payment_account" and "address" variables. Some variable were obfuscated to anonymize the user to protect privacy.


The dimensions are: bids(**7.66m x 9**), test(**4701 x 3**) and train(**2014 x 4**). From the number of lines in the bids dataset we may infer that each line corresponds to a transaction at a given time point. 

The train and test dataset contain information about the bidder while the bids contains information about each bid in an auction. Thus, we must extract the feature from a merged dataset between bids and bidders. As stated before, this is a binary classification task and the main challenge here, in my opinion, is to identify from the raw variables or features(derived from the raw variables) which ones are important for the classification. Thus, the first heavy part is data pre-processing for feature creation.


# Human vs Robot: Exploratory data analysis
Since in the train data we have some bots(with a degree of uncertainty), we might explore this dataset in order to get some clarification and directions. The key assumption here is that bot and human behavior found in the training will also be present in the test/bids data. Another important assumption is that both the information is the train set is representative of the overall population(bidders, humans and bots).



Firstly, we start by looking at how many bidders we have in all 3 data sets:
```{r}
length(unique(bids.data$bidder_id))# ok with kaggle
length(unique(test.data$bidder_id))# ok with kaggle
length(unique(train.data$bidder_id))# ok with kaggle
```

We start by extracting from training dataset bots and humans. 

```{r}
# selecting bots and humans from train data
bots.train.data <- dplyr::filter(train.data, outcome == 1)
humans.train.data <- dplyr::filter(train.data, outcome == 0)

total.train.data <- c(
  dim(dplyr::distinct(humans.train.data))[1],
  dim(dplyr::distinct(bots.train.data))[1]
)
```



```{r,echo=FALSE}
plot <- barplot(total.train.data, ylim = c(0, max(total.train.data) + 500), names.arg = c("Humans","Bots"),main = "Number of each type of user in the training dataset")
text(x = plot, y = total.train.data + 100, labels = paste("n = ", total.train.data, sep = ""))
```

As expected, we have more humans than bots. Now, we perform an inner join between the stratified training and the bid datasets to retrieve only the corresponding rows to those users that are in both datasets. This allows us to focus, for now, only on already classified users. 


```{r}
# now, selecting from the bid dataset only the bids made by bots
bots.bids.data <- dplyr::inner_join(bids.data, bots.train.data, by = "bidder_id")
humans.bids.data <- dplyr::inner_join(bids.data, humans.train.data, by = "bidder_id")
dim(bots.bids.data)
dim(humans.bids.data)

dim(bots.bids.data)[1] + dim(humans.bids.data)[1]
dim(bids.data)
# obviously, not all entries are classified

### Classifield bids
class.bids.data <- bind_rows(humans.bids.data, bots.bids.data)
# same as dplyr::inner_join(bids.data, train.data, by = "bidder_id")
glimpse(class.bids.data)
# class.bids.data %>% janitor::tabyl(auction)


######### Analysing bidder_id 
# quantity of bids per bidder_id
# feature
total.bid.class.bids.data <- class.bids.data %>%
  dplyr::group_by(bidder_id) %>%
  summarise(totalbids = n()) %>%
  inner_join(distinct(select(class.bids.data, bidder_id, outcome)), by = "bidder_id")

# plotting the quantity of bids per type of user
total.bids.perclass <- total.bid.class.bids.data %>%
  dplyr::group_by(outcome) %>%
  summarise(sum(totalbids))
plot <- barplot(total.bids.perclass$`sum(totalbids)`, ylim = c(0, max(total.bids.perclass$`sum(totalbids)`) + 150000), names.arg = c("Humans","Bots"))
text(x = plot, y = total.bids.perclass$`sum(totalbids)` + 100000, labels = paste("n = ", total.bids.perclass$`sum(totalbids)`, sep = ""))
```

From this new subset, we can see that the majority of bids were placed by humans. This is something as an expected behavior as we have more humans than bots. Next, we take a look at the number of bids made by each user in both subsets. 

```{r}
par(mfrow=c(1,2))
barplot(filter(total.bid.class.bids.data, outcome==0)$totalbids,xlab = "Bidder (human)",ylab = "Number of bids",ylim = c(0,500000))
barplot(filter(total.bid.class.bids.data, outcome==1)$totalbids,xlab = "Bidder (robot)",ylab = "Number of bids",ylim = c(0,500000))
```


From the plot, we see some heavy bidders on both sides but it is complicated to draw any conclusions. Let's also check some statistics of this feature:
```{r}
aux <- cbind(EstatDescr(filter(total.bid.class.bids.data, outcome==0)$totalbids,coluna = F),EstatDescr(filter(total.bid.class.bids.data, outcome==1)$totalbids,coluna = F))
colnames(aux) <- c("Human","Robot")
aux
```

Now, we have something to discuss. By comparing side by side the statistics, we start to see how different both groups are. We see that apart from the minimum and  maximum number of bids, robot's statistics are much bigger than humans statistics. This indicates that robots bid more in spite of being in less number. How are the heavy users affecting the statistics? Let's see a robust measure of the standard deviation, for example.

```{r}
robustbase::Qn(filter(total.bid.class.bids.data, outcome==0)$totalbids)
robustbase::Qn(filter(total.bid.class.bids.data, outcome==1)$totalbids)
```
The Qn robust standard deviation is much smaller than both standard deviations. Thus, the __total number of bids of each user__ may be interesting to be considered in the modeling.

Now, focusing on the auctions. There is a total of 12740 different auctions where both humans and bots bid. Let's calculate some statistics related to the number of bids for each auction.
```{r}
total.auctions <- length(unique(class.bids.data$auction))
bids.per.auction <- class.bids.data %>%
  dplyr::group_by(auction) %>%
  summarise(total = n())
 EstatDescr(bids.per.auction$total)
```

We see that the average number of bid per auction is 241. Although this can be an indicator of the overall performance of the site across this merged dataset of humans and bot, it doesn't help us to classify bots and humans.

Let's check the total number of participation in auctions per user.

```{r}
total.auctions.partipation.per.user <- class.bids.data %>%  group_by(bidder_id,outcome) %>% summarise(total=n_distinct(auction))
par(mfrow = c(1, 2))
plot(filter(total.auctions.partipation.per.user, outcome == 0)$total, xlab = "Bidder", ylab = "Number of auctions", ylim = c(0, 2000),main="Humans",type="p",cex=0.8)
plot(filter(total.auctions.partipation.per.user, outcome == 1)$total, xlab = "Bidder", ylab = "Number of auctions", ylim = c(0, 2000),main="Robots",type="p",cex=0.8)
aux <- cbind(EstatDescr(filter(total.auctions.partipation.per.user, outcome == 0)$total, coluna = F), EstatDescr(filter(total.auctions.partipation.per.user, outcome == 1)$total, coluna = F))
colnames(aux) <- c("Human", "Robot")
aux
```

We see that, in general, bots will participate in more auctions than humans. Thus, __total number of participation in auctions per user__ may be a feature. We need to verify how each bidder bids per auction, on average, for both classes.


```{r}
# average number of bids per user
# feature
avg.bids.per.user <- class.bids.data %>%
  dplyr::group_by(bidder_id, auction) %>%
  summarise(numberbids = n()) %>%
  dplyr::group_by(bidder_id) %>%
  summarise(meanbids = mean(numberbids)) %>%
  inner_join(distinct(select(class.bids.data, bidder_id, outcome)), by = "bidder_id")
par(mfrow=c(1,2))
barplot(filter(avg.bids.per.user, outcome==0)$meanbids,xlab = "Bidder (human)",ylab = "Number of bids",ylim = c(0,1000))
barplot(filter(avg.bids.per.user, outcome==1)$meanbids,xlab = "Bidder (robot)",ylab = "Number of bids",ylim = c(0,1000))
aux <- cbind(EstatDescr(filter(avg.bids.per.user, outcome==0)$meanbids,coluna = F),EstatDescr(filter(avg.bids.per.user, outcome==1)$meanbids,coluna = F))
colnames(aux) <- c("Human","Robot")
aux
```

This is cool! From the plots, we can see that we have some humans bidding a lot! Again, these quantities are the average number of bids per user. Are they addicted or bots?
From the statistics, we see that, in general, bots bids much more than humans. This is expected behavior as bots should bid more to secure the win. This hypothesis may be verified considering the time when the auction is over. Maybe there is some way to establish a connection between the time and the number of bids... The __average number of bids per user__ is an interesting feature. 



The next variable in the dataset is merchandise. The question here is: do humans and robots have a favorite merchandise category where they come from? 
```{r}
total.auctions.per.category <-class.bids.data %>% group_by(merchandise) %>% summarise(total=n())
total.auctions.per.category %>%   arrange(desc(total), .by_group = TRUE)
```

Let's check how humans and bots are coming from each merchandise.

```{r}
total.auctions.per.category.per.user <- class.bids.data %>%
  dplyr::group_by(merchandise, outcome) %>%
  summarise(total = n())

total.auctions.per.category.per.user %>%
  filter(outcome == 0) %>%
  full_join(total.auctions.per.category.per.user %>% filter(outcome == 1), by = "merchandise")
```

Here, we see that bots are coming after sporting goods, mobile and jewelry. There are no presence of bots coming after auto parts, furniture nor clothing.

Now, we take a look at how many humans and bots a coming after a particular merchandise. 
```{r}
# most common merchandise per user
most.common.merch.per.user <- class.bids.data %>%
  dplyr::group_by(bidder_id, merchandise) %>%
  summarise(number = n()) %>%
  dplyr::group_by(bidder_id) %>%
  summarise(common.merchandise = merchandise[which.max(number)]) %>%
  inner_join(distinct(select(class.bids.data, bidder_id, outcome)), by = "bidder_id")


tab.aux <- most.common.merch.per.user %>%
  dplyr::group_by(common.merchandise, outcome) %>%
  summarise(total = n())

tab.aux %>%
  filter(outcome == 0) %>%
  full_join(tab.aux %>% filter(outcome == 1), by = "common.merchandise")
```

This last table measures the most frequent merchandise of each use. We see that mobile and sporting goods are the main merchandise of 27 and 32 distinct bots, respectively. Maybe we shouldn't select any feature related to the merchandise. 



There is a total of 5729 different devices. The hypothesis here is to verify how many different devices each bot have and it this quantity is much different for humans.


```{r}
most.common.phone.per.user <- class.bids.data %>%
  dplyr::group_by(bidder_id, device) %>%
  summarise(number = n()) %>%
  dplyr::group_by(bidder_id) %>%
  summarise(common.phone = device[which.max(number)]) %>%
  inner_join(distinct(select(class.bids.data, bidder_id, outcome)), by = "bidder_id")
tab.aux <- most.common.phone.per.user %>% group_by(common.phone,outcome) %>% summarise(total=n())
par(mfrow=c(1,2))
plot.aux <- (tab.aux %>% filter(outcome==0))$total
plot(plot.aux,ylab="Number of users",main="Humans",xlab="Device nº")
plot.aux <- (tab.aux %>% filter(outcome==1))$total
plot(plot.aux,ylab="Number of users",main="Robots",xlab="Devices nº")
```
 
This part is done from the device point of view aiming to answer: Given a device, how popular is this device among robots and humans? We observe that although humans have more different types of preferred devices, some of these devices are highly used by a large number of humans. Are the preferred phones the same for bots and humans?

```{r}
# checking the most common devices in each group sorted by device name
head(tab.aux %>% filter(outcome==0) %>% arrange(desc(total)),10) %>% arrange(desc(common.phone))
head(tab.aux %>% filter(outcome==1) %>% arrange(desc(total)),10) %>% arrange(desc(common.phone))
```

We see that most of the most preferred phones are the same among bot and users. As it is expected for the bot to mimic human behavior. Maybe what we should be looking is at the number of different devices per user.

```{r}
number.devices.per.user <- class.bids.data %>% dplyr::group_by(bidder_id,outcome) %>% summarise(total = n_distinct(device))
par(mfrow=c(1,2))
plot.aux <- (number.devices.per.user %>% filter(outcome == 0))$total
plot(plot.aux, ylab = "Number of phones", main = "Humans", xlab = "Bidder")
plot.aux <- (number.devices.per.user %>% filter(outcome == 1))$total
plot(plot.aux, ylab = "Number of phones", main = "Robots", xlab = "Bidder")
```

The picture is still unclear. We have from the human side users with as many as 500 different devices. Let's take a look at the statistics

```{r}
aux <- cbind(EstatDescr(filter(number.devices.per.user, outcome == 0)$total, coluna = F), EstatDescr(filter(number.devices.per.user, outcome == 1)$total, coluna = F))
colnames(aux) <- c("Human", "Robot")
aux
```

We see that apart from the outlying users from both sides, bots tend to use more different devices than humans. Since the 3rd quantile of the number of devices is 52, probably, if a user has a much larger value than this, it is a bot. Thus, the __number of different devices__ used by the user can be considered as a feature.

How are the devices(unique) dispersed over the auctions per classes?


```{r}
number.distinc.devices.per.auction <- class.bids.data %>% dplyr::group_by(auction,outcome) %>% summarise(total=n_distinct(device))
par(mfrow = c(1, 2))
plot.aux <- (number.distinc.devices.per.auction %>% filter(outcome == 0))$total
plot(plot.aux, ylab = "Number of devices", main = "Humans", xlab = "Auction nº")
plot.aux <- (number.distinc.devices.per.auction %>% filter(outcome == 1))$total
plot(plot.aux, ylab = "Number of devices", main = "Robots", xlab = "Auction nº")
```

These graphic tell us in the auctions humans have much more different devices than bots. This is an agreement with the hypothesis "Humans will have more different devices than robots". In order to have a more clear view, let's verify how many different devices each bidder have stratified per auction, on average. If a user is human, we expect that this number will be small.

```{r}
# statistics of number of devices per auction per user
avg.dev.auc.per.user <- class.bids.data %>% dplyr::group_by(bidder_id, auction, device,outcome) %>% 
  summarise(n()) %>% 
  dplyr::group_by(bidder_id, auction,outcome) %>%
  summarise(totaldev = n()) %>%
  dplyr::group_by(bidder_id,outcome) %>%
  summarise(avg.dev.auc = mean(totaldev))
aux <- cbind(EstatDescr(filter(avg.dev.auc.per.user, outcome == 0)$avg.dev.auc, coluna = F), EstatDescr(filter(avg.dev.auc.per.user, outcome == 1)$avg.dev.auc, coluna = F))
colnames(aux) <- c("Human", "Robot")
aux
```

Although bot values are not much higher than humans, the statistics indicate that, on average, bots will use more devices per action. The __average number of different devices used per auction__ may be an interesting feature to consider.

Next variable of interest is time. Since the scale of the original values was high, we can apply a log transformation. There is a total of 742669 time points. Let's graph them and check if we can see something.

```{r}
plot.ts(y=1:742669,x=unique(class.bids.data$time))
```

It seems that we the bids are ordered in 3 chunks with well defined start and end times. Maybe each time chunk is a day and probably they are not consecutive days. If these time chunks were consecutive or if we somehow could discover how many time units are the space between them, we could use time series models with unequally spaced data. 


What we know for sure is that the bids are grouped over auctions. Therefore, for a given audition, we can order the time setting the first bit at 0 time and compute the time difference between each bids.

```{r}
time.diff.bids <- class.bids.data %>%
  group_by(auction) %>%
  do(., differences(.))
```

Having the time between bids, lets calculate the average time between bids and also the proportion of times when the bidder was the first to bid per user. 
```{r}
time.diff.bids %>%
  group_by(bidder_id) %>%
  summarise(average.time.to.bid = mean(time), percent.first = sum(.[["time"]] == 0) / n())
# Average time between bids and also the proportion of times when the bidder was the first to bid
mean.proport.bids <- time.diff.bids %>%
  group_by(bidder_id,outcome) %>%
  summarise(average.time.to.bid = mean(time), percent.first = sum(.[["time"]] == 0) / n())

aux <- cbind(EstatDescr(filter(mean.proport.bids, outcome == 0)$average.time.to.bid, coluna = F), EstatDescr(filter(mean.proport.bids, outcome == 1)$average.time.to.bid, coluna = F))
colnames(aux) <- c("Human", "Robot")
aux


aux <- cbind(EstatDescr(filter(mean.proport.bids, outcome == 0)$percent.first, coluna = F), EstatDescr(filter(mean.proport.bids, outcome == 1)$percent.first, coluna = F))
colnames(aux) <- c("Human", "Robot")
aux
```

From the statistics we can see that bots have a smaller time between bids and, in general, are not the first to bid. Let's check if they are the last to bid.

```{r}
percent.final <- time.diff.bids %>%
  group_by(auction) %>%
  mutate(last = c(rep(0, n() - 1), 1)) %>%
  group_by(bidder_id,outcome) %>%
  summarise(percent.final = mean(last))
```

Thus, let's select __the average time between bids and also the proportion of times when the bidder was the first and the last to bid__ as features..


There are 199 different countries in the data set. The top 10 countries from where the bidders came from are

```{r}
# number of distinct bidders per country
number.distinc.user.per.country <- class.bids.data %>% dplyr::group_by(country,outcome) %>% summarise(total=n_distinct(bidder_id)) %>% arrange(desc(total))
# common countries
number.distinc.user.per.country %>%
  filter(outcome == 0) %>%
  full_join(number.distinc.user.per.country %>% filter(outcome == 1), by = "country")
```

Note that in this table, the bidder is only counted once. The bidder might bid from one or more different countries. Thus, as in the device case, we must look at the total number of different countries per user and the average number of countries per auction.



```{r}
number.countries.per.user <- class.bids.data %>% dplyr::group_by(bidder_id,outcome) %>% summarise(total = n_distinct(country))
aux <- cbind(EstatDescr(filter(number.countries.per.user, outcome == 0)$total, coluna = F), EstatDescr(filter(number.countries.per.user, outcome == 1)$total, coluna = F))
colnames(aux) <- c("Human", "Robot")
aux
```

In this case, we see that robots bid much more from different countries than humans. Therefore, the __total number of different countries per user__ is selected.

```{r}
avg.number.countries.per.auction <- class.bids.data %>% dplyr::group_by(bidder_id, auction, country,outcome) %>% 
  summarise(n()) %>% 
  dplyr::group_by(bidder_id, auction,outcome) %>%
  summarise(totalc = n()) %>%
  dplyr::group_by(bidder_id,outcome) %>%
  summarise(avg.country.auc = mean(totalc))
par(mfrow = c(1, 2))
plot.aux <- (avg.number.countries.per.auction %>% filter(outcome == 0))$avg.country.auc
plot(plot.aux, ylab = "Number of countries(avg)", main = "Humans", xlab = "Bidder",type="p",cex=0.8)
plot.aux <- (avg.number.countries.per.auction %>% filter(outcome == 1))$avg.country.auc
plot(plot.aux, ylab = "Number of countries(avg)", main = "Robots", xlab = "Bidder",type="p",cex=0.8)
```

We see that the average number of different countries per auction are almost the same for both groups. To verify this, let's take a look at the statistics.

```{r}
aux <- cbind(EstatDescr(filter(avg.number.countries.per.auction, outcome == 0)$avg.country.auc, coluna = F), EstatDescr(filter(avg.number.countries.per.auction, outcome == 1)$avg.country.auc, coluna = F))
colnames(aux) <- c("Human", "Robot")
aux
```

Apart from some heavy users from the human side, we see that both groups present similar statistics. Thus, we might not consider this feature in our model.

Now, considering the IP, we have 1030950 different address. Maybe robots will have more different IPs than humans. 

```{r}
number.distinc.ip.per.user <- class.bids.data %>% group_by(bidder_id,outcome) %>% summarise(total=n_distinct(ip)) %>% arrange(desc(total))
aux <- cbind(EstatDescr(filter(number.distinc.ip.per.user, outcome == 0)$total, coluna = F), EstatDescr(filter(number.distinc.ip.per.user, outcome == 1)$total, coluna = F))
colnames(aux) <- c("Human", "Robot")
aux
```

As expected, while the median number of distinct IPs of humans is 11, for bots it is 290. Thus, __the number of different IP address__ is selected as a feature. This value might also be different if we check it per auction.

```{r}
avg.number.ip.per.auction <- class.bids.data %>% dplyr::group_by(bidder_id, auction, ip,outcome) %>% 
  summarise(n()) %>% 
  dplyr::group_by(bidder_id, auction,outcome) %>%
  summarise(totalip = n()) %>%
  dplyr::group_by(bidder_id,outcome) %>%
  summarise(avg.ip.auc = mean(totalip))
aux <- cbind(EstatDescr(filter(avg.number.ip.per.auction, outcome == 0)$avg.ip.auc, coluna = F), EstatDescr(filter(avg.number.ip.per.auction, outcome == 1)$avg.ip.auc, coluna = F))
colnames(aux) <- c("Human", "Robot")
aux
```


Indeed, both groups are different.Again, per auction, robots use more IP address than humans. Therefore, __the average number of ip per auction__ can also be considered for the modeling.



To end this EDA, let's check the URL. There are 663873 distinct URL address.
```{r}
# number of distinct url per user
number.distinc.url.per.user <- class.bids.data %>% group_by(bidder_id,outcome) %>% summarise(total=n_distinct(url)) %>% arrange(desc(total))
aux <- cbind(EstatDescr(filter(number.distinc.ip.per.user, outcome == 0)$total, coluna = F), EstatDescr(filter(number.distinc.ip.per.user, outcome == 1)$total, coluna = F))
colnames(aux) <- c("Human", "Robot")
aux
```

The table above presents __the number of distinct URLS per user__. Indeed, robots have much more URLs than humans.


Now, after have looked at the training dataset we redirect our attention towards the bids dataset.

# Feature extraction

Motivated by the above exploratory data analysis, we may select the following features to be inserted into our classification model

   + Total number of bids per user
   + Total number of participated auctions
   + Average number of bids per user per auction
   + Number of different devices used by the user
   + Most used device by the user
   + Average number of different devices used per auction
   + Total number of different countries
   + Average number of different countries
   + Average time to bid
   + Proportion of times when the user was the first and the last to bid
   + Total number of different ips
   + Average number of different ips

After deciding the features that we are going to use as predictors, lets calculate them for the bids-train merged dataset.

```{r}
# source("functions.R")
# feature.x <- create.x(bids.data)
# dim(feature.x)
# # merging feature and train data
# dim(train.data)
# data.x <- inner_join(feature.x,train.data %>% select(bidder_id,outcome),by="bidder_id")
# summary(data.x)
# 
# # transforming the outcome in factors
# data.x$outcome<-as.factor(data.x$outcome)
# levels(data.x$outcome)<-c("human","bot")
```



# Modeling
The problem and the dataset come from a Kaggle competition. If we were to take part in the competition we would seek out the best model(which could also be an ensemble of multiple classification methods) and use it to predict the outcome(robot or human) in the test data. Taking this approach is computationally expensive. That is, the seek of which would require us to run some algorithms in clusters or even have to parallelize some of them. There is also an important part of tuning the hyper parameters of the selected methods. These two tasks together with the selection of right features take time and performing them is what lead to a top scorer in the leader board.

However, as the competition is over, we take the modeling part following a discussion of why we are considering some specific models. In this part, one important metric is the run-time of the employed classification methods with a trade-off of the model performance, measured by the area under the receiver operating characteristic curve (AUROC). To be short, AUROC relates in a single measure the true positive and false positive rates and the closer to 1 the better.

Thus, after feature extraction and computation, we need to implement our classification method. There are many classification methods available in the machine learning literature ranging from simple logistic regression to neural networks with many hidden layers and some methods perform better than others depending of the nature of the dataset.



There are several ways of running a machine learning algorithm in R. Nowadays, an elegant approach is to consider the package caret which implements several good practices allowing fine tuning control of the hyper parameters of different models. We start by setting our control parameters, we utilize 25-fold for cross-validation repeated 5 times. This allow us to have a good balance in CPU time consumption.

```{r}
# control.tun <-trainControl(method = "repeatedcv", number = 25,repeats = 5,savePredictions = 'final',classProbs = TRUE)
```

The first model is the logistic regression model

```{r}
# model <- train(outcome ~ total.bids + total.auc + avg.bids + total.dist.dev + avg.dev.auc + total.country + avg.country.auc +  average.time.to.bid   +    percent.first + percent.final
# +total.dist.ip + avg.ip.auc, data=data.x,method="glm",family=binomial(),trControl=control.tun)
```

After the execution several warnings are shown. This is due to the data and to the underlying optimization method(NR or Fisher scoring). Another fact is also the high correlation of the variables, as we can see from the correlation matrix.


```{r}
# cor(data.x[,2:(dim(data.x)[2]-1)])
```
To solve this issue we could apply a PCA transformation, however, we would loose direct interpretation of the input features. This model leads


The second model if a random forest model.

```{r}
# model2<-train(outcome ~ total.bids + total.auc + avg.bids + total.dist.dev + avg.dev.auc + total.country + avg.country.auc +  average.time.to.bid   +    percent.first + percent.final
#              +total.dist.ip + avg.ip.auc, data=data.x, method = "rf", trControl=control.tun)
```

The last model is gradient boosted model.
```{r}
# model3<-train(outcome ~ total.bids + total.auc + avg.bids + total.dist.dev + avg.dev.auc + total.country + avg.country.auc +  average.time.to.bid   +    percent.first + percent.final
#               +total.dist.ip + avg.ip.auc, data=data.x, method = "gbm", trControl=control.tun,verbose=F)
```


After training our classifiers, let's load the test data

```{r}
# # 4630 |= 4700, there are 70 bidders on our dataset without any bidding activity
# test.data.x <- left_join(test.data %>% select(bidder_id),feature.x,by="bidder_id")
# # summary(test.data.x)
# # # extracting all the users with no bids and attributing 0 to their prediction
# no.activity.bidder <- test.data.x %>% filter(is.na(total.bids)==TRUE) %>% select(bidder_id) %>% mutate(prediction=0)
# # no.activity.bidder
# # 
# # # test.data.x <- inner_join(test.data %>% select(bidder_id),feature.x,by="bidder_id")
# test.data.x <- inner_join(test.data %>% select(bidder_id),feature.x,by="bidder_id")
# summary(test.data.x)
# dim(test.data.x)
```

There are 70 bidders on our dataset without any bidding activity, so we had to perform an inner join to exclude them. They'll be reinserted before submitting the results to the leader board.

```{r}
# pred.bot <-predict(model,test.data.x[,2:dim(test.data.x)[2]])
# pred <- data.frame("bidder_id" = test.data.x$bidder_id,"prediction" = pred.bot)
# results <- rbind(pred,no.activity.bidder)
# # ordering the results as in test file
# aux <- results %>% arrange(match(bidder_id, test.data$bidder_id))
# write.csv(aux, "LOG25CV10R.csv", row.names = FALSE)
```

The logistic regression achieved 0.82308 and 0.81637 for private and public scores, respectively.


Results for the random forest model
```{r}
# pred.bot <-predict(model2,test.data.x[,2:dim(test.data.x)[2]], type = "prob")$bot
# pred <- data.frame("bidder_id" = test.data.x$bidder_id,"prediction" = pred.bot)
# results <- rbind(pred,no.activity.bidder)
# aux <- results %>% arrange(match(bidder_id, test.data$bidder_id))
# write.csv(aux, "RF25CV10R.csv", row.names = FALSE)
```

RF achieved 0.90236 and 0.87893, respectively.


```{r}
# pred.bot <-predict(model3,test.data.x[,2:dim(test.data.x)[2]], type = "prob")$bot
# pred <- data.frame("bidder_id" = test.data.x$bidder_id,"prediction" = pred.bot)
# results <- rbind(pred,no.activity.bidder)
# aux <- results %>% arrange(match(bidder_id, test.data$bidder_id))
# write.csv(aux, "GBM25CV10R.csv", row.names = FALSE)
```

With scores of 0.89655 and 0.86872. To end this analysis, let's try to average the models

```{r}
# avg.pred.bot <- apply(cbind(predict(model2,test.data.x[,2:dim(test.data.x)[2]], type = "prob")$bot,predict(model3,test.data.x[,2:dim(test.data.x)[2]], type = "prob")$bot),1,mean)
# res.avg.pred.bot <- data.frame("bidder_id" = test.data.x$bidder_id,"prediction" = avg.pred.bot)
# results <- rbind(res.avg.pred.bot,no.activity.bidder)
# aux.avg <- results %>% arrange(match(bidder_id, test.data$bidder_id))
# write.csv(aux.avg, "avg25r10submit.csv", row.names = FALSE)
```

After submitting this last csv to Kaggle we got a Private=0.89995 and Public=0.88266 scores. Since all models run equivalently, we would stick to the RF model. However, this model can be  definitely improved.


# Where can we improve it?

As stated before, the objective was to perform a fundamental approach for solving the classification problem. For sure, the first place to improve without writing a lot of code is to consider different tuning hyper parameters for the models. The second is to consider a different classification methods with different tuning parameters, for example, SVM and Bayesian models. We can also explore and extract more features, specially features related to the time variable. We can also apply PCA to solve the multicolinearity while reducing the dimension to see if the correlation is harming the models.


